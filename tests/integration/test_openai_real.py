"""OpenAI APIå®Ÿçµ±åˆãƒ†ã‚¹ãƒˆã€‚

å®Ÿéš›ã®OpenAI APIã‚’ä½¿ç”¨ã—ã¦RAGã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œã‚’æ¤œè¨¼ã™ã‚‹ã€‚
ç’°å¢ƒå¤‰æ•°OPENAI_API_KEYãŒå¿…è¦ã€‚
"""

import os
from uuid import uuid4

import pytest
from dotenv import load_dotenv

from src.domain.entities.rag_query import RAGQuery
from src.domain.value_objects import DocumentId, SearchResultItem
from src.domain.value_objects.rag_context import RAGContext
from src.infrastructure.externals.llms import OpenAILLMService
from src.infrastructure.externals.rag import RAGServiceImpl

# .env.testãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv(".env.test", override=True)


@pytest.mark.skipif(
    not os.getenv("OPENAI_API_KEY")
    or os.getenv("OPENAI_API_KEY") == "your-openai-api-key-here",
    reason="OpenAI API key not configured in .env.test",
)
class TestOpenAIRealIntegration:
    """OpenAI APIå®Ÿçµ±åˆãƒ†ã‚¹ãƒˆã€‚"""

    @pytest.fixture
    def sample_search_results(self) -> list[SearchResultItem]:
        """ãƒ†ã‚¹ãƒˆç”¨ã®æ¤œç´¢çµæœã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
        return [
            SearchResultItem(
                document_id=DocumentId(value=str(uuid4())),
                document_title="äººå·¥çŸ¥èƒ½ã®æ­´å²",
                content_preview="äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã®ç ”ç©¶ã¯1950å¹´ä»£ã«å§‹ã¾ã‚Šã¾ã—ãŸã€‚ã‚¢ãƒ©ãƒ³ãƒ»ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ãŒã€Œæ©Ÿæ¢°ã¯æ€è€ƒã§ãã‚‹ã‹ã€ã¨ã„ã†å•ã„ã‚’æŠ•ã’ã‹ã‘ã€ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆã‚’ææ¡ˆã—ã¾ã—ãŸã€‚",
                score=0.95,
                match_type="both",
                chunk_id="chunk1",
                chunk_index=0,
            ),
            SearchResultItem(
                document_id=DocumentId(value=str(uuid4())),
                document_title="æ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤",
                content_preview="æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã€äºˆæ¸¬ã‚„åˆ†é¡ã‚’è¡Œã†æŠ€è¡“ã§ã™ã€‚æ•™å¸«ã‚ã‚Šå­¦ç¿’ã€æ•™å¸«ãªã—å­¦ç¿’ã€å¼·åŒ–å­¦ç¿’ã®3ã¤ã®ä¸»è¦ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã‚ã‚Šã¾ã™ã€‚",
                score=0.85,
                match_type="both",
                chunk_id="chunk2",
                chunk_index=1,
            ),
            SearchResultItem(
                document_id=DocumentId(value=str(uuid4())),
                document_title="æ·±å±¤å­¦ç¿’ã®é€²åŒ–",
                content_preview="æ·±å±¤å­¦ç¿’ã¯ã€å¤šå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ã®ä¸€åˆ†é‡ã§ã™ã€‚ç”»åƒèªè­˜ã€éŸ³å£°èªè­˜ã€è‡ªç„¶è¨€èªå‡¦ç†ãªã©ã§é©å‘½çš„ãªæˆæœã‚’ä¸Šã’ã¦ã„ã¾ã™ã€‚",
                score=0.75,
                match_type="both",
                chunk_id="chunk3",
                chunk_index=2,
            ),
        ]

    @pytest.fixture
    def rag_context(self, sample_search_results: list[SearchResultItem]) -> RAGContext:
        """ãƒ†ã‚¹ãƒˆç”¨ã®RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
        unique_docs = len({item.document_id for item in sample_search_results})
        max_score = max((item.score for item in sample_search_results), default=0.0)

        return RAGContext(
            query_text="äººå·¥çŸ¥èƒ½ã®æ­´å²ã¨ç™ºå±•ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
            search_results=sample_search_results,
            context_text="",
            total_chunks=len(sample_search_results),
            unique_documents=unique_docs,
            max_relevance_score=max_score,
            metadata={"search_type": "hybrid"},
        )

    @pytest.fixture
    def test_query(self) -> RAGQuery:
        """ãƒ†ã‚¹ãƒˆç”¨ã®RAGã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
        return RAGQuery(
            query_text="äººå·¥çŸ¥èƒ½ã®æ­´å²ã¨ç™ºå±•ã«ã¤ã„ã¦ã€ä¸»è¦ãªå‡ºæ¥äº‹ã‚’å«ã‚ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            search_type="hybrid",
            max_results=5,
            temperature=0.7,
            include_citations=True,
        )

    @pytest.mark.asyncio
    async def test_openai_basic_query(
        self,
        test_query: RAGQuery,
        rag_context: RAGContext,
    ) -> None:
        """OpenAI APIã§åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã€‚"""
        # OpenAI LLMã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œæˆ
        llm_service = OpenAILLMService(
            api_key=os.getenv("OPENAI_API_KEY", ""),
            model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
        )

        # RAGã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œæˆ
        rag_service = RAGServiceImpl(llm_service=llm_service)

        # å›ç­”ã‚’ç”Ÿæˆ
        answer = await rag_service.process_query(
            query=test_query,
            context=rag_context,
        )

        # æ¤œè¨¼
        assert answer.answer_text, "å›ç­”ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™"
        assert len(answer.answer_text) > 100, "å›ç­”ãŒçŸ­ã™ãã¾ã™"
        assert answer.model_name.startswith(
            "gpt-3.5-turbo"
        )  # OpenAIã¯å…·ä½“çš„ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã‚’è¿”ã™

        # æ—¥æœ¬èªã®å›ç­”ã«ãªã£ã¦ã„ã‚‹ã‹ç¢ºèª
        japanese_chars = set(
            "ã‚ã„ã†ãˆãŠã‹ããã‘ã“ã•ã—ã™ã›ããŸã¡ã¤ã¦ã¨ãªã«ã¬ã­ã®ã¯ã²ãµã¸ã»ã¾ã¿ã‚€ã‚ã‚‚ã‚„ã‚†ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚ã‚’ã‚“"
        )
        has_japanese = any(char in japanese_chars for char in answer.answer_text)
        assert has_japanese, "å›ç­”ã«æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“"

        # å¼•ç”¨ã®æ¤œè¨¼
        if answer.citations:
            for citation in answer.citations:
                assert citation.document_id
                assert citation.document_title
                assert citation.relevance_score >= 0.0

        print("\nâœ… å›ç­”ç”ŸæˆæˆåŠŸ:")
        print(f"- æ–‡å­—æ•°: {len(answer.answer_text)}")
        print(f"- å¼•ç”¨æ•°: {len(answer.citations)}")
        print(f"- ãƒ¢ãƒ‡ãƒ«: {answer.model_name}")
        print("\nå›ç­”å†…å®¹ï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰:")
        print(answer.answer_text[:200] + "...")

    @pytest.mark.asyncio
    async def test_openai_streaming_response(
        self,
        test_query: RAGQuery,
        rag_context: RAGContext,
    ) -> None:
        """OpenAI APIã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã€‚"""
        # OpenAI LLMã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œæˆ
        llm_service = OpenAILLMService(
            api_key=os.getenv("OPENAI_API_KEY", ""),
            model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
        )

        # RAGã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œæˆ
        rag_service = RAGServiceImpl(llm_service=llm_service)

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’å–å¾—
        chunks: list[str] = []
        async for chunk in rag_service.stream_answer(
            query=test_query,
            context=rag_context,
        ):
            chunks.append(chunk)

        # æ¤œè¨¼
        assert len(chunks) > 0, "ãƒãƒ£ãƒ³ã‚¯ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"
        full_response = "".join(chunks)
        assert len(full_response) > 100, "ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ãŒçŸ­ã™ãã¾ã™"

        print("\nâœ… ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æˆåŠŸ:")
        print(f"- ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")
        print(f"- åˆè¨ˆæ–‡å­—æ•°: {len(full_response)}")

    @pytest.mark.asyncio
    async def test_openai_with_different_temperatures(
        self,
        rag_context: RAGContext,
    ) -> None:
        """ç•°ãªã‚‹æ¸©åº¦è¨­å®šã§ã®OpenAIå¿œç­”ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã€‚"""
        llm_service = OpenAILLMService(
            api_key=os.getenv("OPENAI_API_KEY", ""),
            model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
        )
        rag_service = RAGServiceImpl(llm_service=llm_service)

        temperatures = [0.0, 0.5, 1.0]
        responses = []

        for temp in temperatures:
            query = RAGQuery(
                query_text="AIã®ä¸»è¦ãªå¿œç”¨åˆ†é‡ã‚’3ã¤æŒ™ã’ã¦ãã ã•ã„ã€‚",
                temperature=temp,
                max_results=3,
            )

            answer = await rag_service.process_query(query, rag_context)
            responses.append(answer.answer_text)

            print(f"\næ¸©åº¦ {temp}: {answer.answer_text[:100]}...")

        # æ¸©åº¦0.0ã®å¿œç­”ã¯æ±ºå®šçš„ã§ã‚ã‚‹ã¹ã
        assert responses[0], "æ¸©åº¦0.0ã®å¿œç­”ãŒç©ºã§ã™"

        # ã™ã¹ã¦ã®å¿œç­”ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        for i, resp in enumerate(responses):
            assert len(resp) > 0, f"æ¸©åº¦{temperatures[i]}ã®å¿œç­”ãŒç©ºã§ã™"

    @pytest.mark.asyncio
    async def test_openai_token_usage(
        self,
        test_query: RAGQuery,
        rag_context: RAGContext,
    ) -> None:
        """OpenAI APIã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’ç¢ºèªã™ã‚‹ã€‚"""
        llm_service = OpenAILLMService(
            api_key=os.getenv("OPENAI_API_KEY", ""),
            model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
        )
        rag_service = RAGServiceImpl(llm_service=llm_service)

        answer = await rag_service.process_query(test_query, rag_context)

        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®æ¤œè¨¼
        assert answer.token_usage, "ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        assert answer.token_usage.get("prompt_tokens", 0) > 0
        assert answer.token_usage.get("completion_tokens", 0) > 0
        assert answer.token_usage.get("total_tokens", 0) > 0

        print("\nğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡:")
        print(f"- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {answer.token_usage.get('prompt_tokens', 0)}")
        print(f"- å®Œäº†: {answer.token_usage.get('completion_tokens', 0)}")
        print(f"- åˆè¨ˆ: {answer.token_usage.get('total_tokens', 0)}")

        # ã‚³ã‚¹ãƒˆæ¦‚ç®—ï¼ˆGPT-3.5-turboã®å ´åˆï¼‰
        total_tokens = answer.token_usage.get("total_tokens", 0)
        estimated_cost = (total_tokens / 1000) * 0.002  # $0.002 per 1K tokens
        print(f"- æ¨å®šã‚³ã‚¹ãƒˆ: ${estimated_cost:.4f}")


if __name__ == "__main__":
    # ç›´æ¥å®Ÿè¡Œç”¨
    pytest.main([__file__, "-xvs"])
